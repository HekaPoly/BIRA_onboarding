{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EXWxX_lrAasr",
        "CmRBeV2xAlhc",
        "_nXkomwACeDo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Formation Machine Learning\n",
        "\n",
        "Bienvenue dans ce notebook d’exercices pour la formation en Machine Learning !  \n",
        "Cette formation a pour but d’explorer les concepts fondamentaux du machine learning à travers des implémentations pratiques en Python et l’utilisation de bibliothèques populaires comme `scikit-learn`, `numpy`, `pytorch`, etc.\n",
        "\n",
        "## Contenu\n",
        "\n",
        "Voici un aperçu des sujets couverts dans ce notebook :\n",
        "\n",
        "- Régression linéaire\n",
        "- Régression polynomiale\n",
        "- Régression logistique\n",
        "- Arbres de décision, Forêts aléatoires et Gradient Boosting\n",
        "- Réseaux de neurones\n",
        "\n",
        "## Structure typique d’un exercice\n",
        "\n",
        "Chaque exercice suit généralement la structure suivante :\n",
        "\n",
        "1. **Séparation en jeu d’entraînement / validation**\n",
        "2. **Implémentation et entraînement du modèle**\n",
        "3. **Évaluation du modèle avec des métriques pertinentes**\n",
        "\n",
        "## Pré-requis\n",
        "\n",
        "Avant de commencer, assurez‑vous d’avoir :\n",
        "- **Suivi le MOOC** : [Initiez‑vous au Machine Learning](https://openclassrooms.com/fr/courses/8063076-initiez-vous-au-machine-learning) (OpenClassrooms)\n",
        "- **Lu les chapitres 1 à 4** de la série [Neural Networks](https://www.3blue1brown.com/topics/neural-networks) (3Blue1Brown)\n",
        "- **Réalisé le tutoriel** : [PyTorch Basics](https://docs.pytorch.org/tutorials/beginner/basics/intro.html)\n",
        "\n",
        "## Pour aller plus loin\n",
        "\n",
        "Voici quelques ressources pour approfondir vos connaissances en Machine Learning au-delà de ce qui a été présenté dans ce notebook:\n",
        "\n",
        "- [MIT 6.036 - Introduction to Machine Learning](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/) ⭐⭐\n",
        "- [MIT 6.S191 - Introduction to Deep Learning](https://www.youtube.com/watch?v=alfdI7S6wCY&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) ⭐⭐\n",
        "- [MIT 18.657 - Mathematics of Machine Learning](https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/download/) ⭐⭐⭐\n",
        "- [PolyMTL INF8245E - Machine Learning (Vidéos)](https://www.youtube.com/watch?v=-6ChHxllZVU&list=PLImtCgowF_ETupFCGQqmvS_2nqErZbifm) ⭐⭐\n",
        "- [PolyMTL INF8245E - Machine Learning (Notes)](https://drive.google.com/drive/folders/1xUqzxJK5NbAxUZOInpTBAAQk8iwKSKVS) ⭐⭐\n",
        "- [PolyMTL INF8359DE - Reinforcement Learning](https://www.youtube.com/watch?v=J9JZyyPCJcQ&list=PLImtCgowF_ES_JdF_UcM60EXTcGZg67Ua) ⭐⭐\n",
        "- [PolyMTL MTH3302 - Méthodes probabilistes et statistiques pour l'IA](https://github.com/decorJim/mth3302) ⭐⭐⭐\n",
        "- [An Introduction to Statistical Learning — Gareth James, Daniela Witten, Trevor Hastie & Robert Tibshirani](https://www.statlearning.com/) ⭐⭐\n",
        "- [Statistical Learning Theory — Vladimir N. Vapnik](https://www.wiley.com/en-us/Statistical-Learning-Theory-9780471030034) ⭐⭐⭐\n",
        "\n",
        "Légende:\n",
        "- ⭐: pas (ou peu) de pré‑requis en ML/programmation/maths.\n",
        "- ⭐⭐: notions de base acquises; à l’aise avec Python + algèbre linéaire + probabilités.\n",
        "- ⭐⭐⭐: solide bagage mathématique (analyse, probabilités, statistiques, optimisation) et envie de rigueur théorique."
      ],
      "metadata": {
        "id": "fkfMuHBm4-ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "r2acU8ZtWoNX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UCKOK6-649D9"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.datasets import fetch_california_housing, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    root_mean_squared_error,\n",
        "    r2_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Régression Linéaire\n",
        "\n",
        "Dans cet exercice, vous allez créer un modèle de machine learning pour **prédire la valeur médiane des maisons (`target`, en centaines de milliers $) à partir du revenu médian (`MedInc`) d’un district californien**.  \n",
        "Vous ne disposerez que d’une seule variable explicative, ce qui facilitera la visualisation de la droite de régression.\n",
        "\n",
        "### Objectifs pédagogiques\n",
        "1. **Comprendre** les étapes essentielles d’un workflow de *machine learning* :  \n",
        "   préparation des données, entraînement, validation et visualisation.  \n",
        "2. **Implémenter** les fonctions `train`, `eval` et `main` afin de :  \n",
        "   - séparer le jeu de données en `train` / `validation` (`train_test_split`) ;  \n",
        "   - entraîner un modèle `LinearRegression` de *scikit-learn* ;  \n",
        "   - calculer et afficher les métriques : **RMSE**, **MSE** et **R²** ;  \n",
        "   - récupérer le coefficient et l’ordonnée à l’origine appris.  \n",
        "3. **Interpréter** la pente obtenue pour relier le revenu aux prix des maisons et discuter de la qualité de la prédiction.\n",
        "\n",
        "### Jeu de données\n",
        "Nous utiliserons le **California Housing Dataset** disponible dans *scikit-learn*.\n",
        "\n",
        "*Référence utile : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html*"
      ],
      "metadata": {
        "id": "a4jchSv7WZWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code fourni"
      ],
      "metadata": {
        "id": "0tNDallDXavH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(model: LinearRegression, X: np.ndarray, y: np.ndarray):\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.scatter(X, y, s=10, alpha=0.4, label=\"Données brutes\")\n",
        "    x_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
        "    y_line = model.predict(x_line)\n",
        "    plt.plot(x_line, y_line, color=\"red\", lw=2, label=\"Droite de régression\")\n",
        "    plt.xlabel(\"Revenu médian\")\n",
        "    plt.ylabel(\"Prix médian d'un maison\")\n",
        "    plt.title(\"Le marché immobilier californien - régression linéaire simple\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UcrY-Y9fXZ-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code à compléter"
      ],
      "metadata": {
        "id": "aQEYKHYYXiwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train: np.ndarray, y_train: np.ndarray) -> LinearRegression:\n",
        "    \"\"\"\n",
        "    Entraîne un modèle de régression linéaire sur les données fournies.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : np.ndarray\n",
        "        Matrice des variables explicatives d'entraînement,\n",
        "        de forme (n_samples, n_features).\n",
        "    y_train : np.ndarray\n",
        "        Vecteur des valeurs cibles d'entraînement,\n",
        "        de forme (n_samples,).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : LinearRegression\n",
        "        Modèle de régression linéaire ajusté sur les données.\n",
        "    \"\"\"\n",
        "    # TODO: Effectuer la régression linéaire sur l'ensemble d'entraînement\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def eval(model: LinearRegression, X_val: np.ndarray, y_val: np.ndarray) -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Évalue les performances du modèle sur un ensemble de validation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : LinearRegression\n",
        "        Modèle de régression linéaire préalablement entraîné.\n",
        "    X_val : np.ndarray\n",
        "        Matrice des variables explicatives de validation,\n",
        "        de forme (n_samples, n_features).\n",
        "    y_val : np.ndarray\n",
        "        Vecteur des valeurs cibles de validation,\n",
        "        de forme (n_samples,).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    metrics : dict[str, float]\n",
        "        Dictionnaire contenant les métriques, où les clés sont :\n",
        "\n",
        "        - ``rmse`` : erreur quadratique moyenne racine.\n",
        "        - ``mse``  : erreur quadratique moyenne.\n",
        "        - ``r2``   : coefficient de détermination R².\n",
        "    \"\"\"\n",
        "    # TODO: Effectuer la validation du modèle sur l'ensemble de validation\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def main(test_ratio: float = 0.2, random_state: int = 0):\n",
        "    data = fetch_california_housing()\n",
        "    X = data.data[:, [data.feature_names.index(\"MedInc\")]]\n",
        "    y = data.target\n",
        "\n",
        "    # TODO: Séparer le jeu de données en ensembles d'entraînement et de validation\n",
        "\n",
        "    # TODO: Instancier et entraîner votre modèle et effectuer la validation du modèle\n",
        "\n",
        "    # TODO: Afficher les métriques, les coefficients et l'ordonnée à l'origine\n",
        "    #       du modèle\n",
        "\n",
        "    # TODO: Afficher la visualisation du jeu de données avec la droite de\n",
        "    #       régression linéaire à l'aide de la fonction plot\n",
        "    raise NotImplementedError()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "e9sVodcu5Tn3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "7a7284e5-d414-427a-8787-f2afa61c0916"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2090570041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2090570041.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(test_ratio, random_state)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# TODO: Afficher la visualisation du jeu de données avec la droite de\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m#       régression linéaire à l'aide de la fonction plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "d50IeWPYCBnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPECTED = {\n",
        "    \"coef\": 0.4203217769894005,\n",
        "    \"intercept\": 0.4432063522765708,\n",
        "    \"rmse\": 0.8494105152406937,\n",
        "    \"mse\": 0.7214982234014606,\n",
        "    \"r2\": 0.4466846804895943,\n",
        "}\n",
        "\n",
        "TOL = 1e-2\n",
        "\n",
        "class TestLinearRegression(unittest.TestCase):\n",
        "    def test_train_learns_coefficients(self):\n",
        "        rng = np.random.default_rng(42)\n",
        "        X = rng.random((300, 1))\n",
        "        true_coef, true_intercept = 4.2, -1.3\n",
        "        y = true_coef * X.squeeze() + true_intercept\n",
        "\n",
        "        model = train(X, y)\n",
        "\n",
        "        self.assertIsInstance(model, LinearRegression)\n",
        "        self.assertAlmostEqual(model.coef_[0], true_coef, delta=TOL)\n",
        "        self.assertAlmostEqual(model.intercept_, true_intercept, delta=TOL)\n",
        "\n",
        "    def test_eval_returns_perfect_metrics(self):\n",
        "        X = np.array([[0.0], [1.0], [2.0], [3.0]])\n",
        "        y = np.array([0.0, 2.0, 4.0, 6.0])\n",
        "\n",
        "        perfect_model = LinearRegression().fit(X, y)\n",
        "        metrics = eval(perfect_model, X, y)\n",
        "\n",
        "        self.assertSetEqual(set(metrics), {\"rmse\", \"mse\", \"r2\"})\n",
        "        self.assertAlmostEqual(metrics[\"rmse\"], 0.0, delta=TOL)\n",
        "        self.assertAlmostEqual(metrics[\"mse\"], 0.0, delta=TOL)\n",
        "        self.assertAlmostEqual(metrics[\"r2\"], 1.0, delta=TOL)\n",
        "\n",
        "    def test_california_exact_values(self):\n",
        "        data = fetch_california_housing()\n",
        "        X = data.data[:, [data.feature_names.index(\"MedInc\")]]\n",
        "        y = data.target\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=0, shuffle=True\n",
        "        )\n",
        "\n",
        "        model = train(X_train, y_train)\n",
        "        metrics = eval(model, X_val, y_val)\n",
        "\n",
        "        self.assertAlmostEqual(model.coef_[0], EXPECTED[\"coef\"], delta=TOL,\n",
        "                               msg=\"Coefficient drift\")\n",
        "        self.assertAlmostEqual(model.intercept_, EXPECTED[\"intercept\"], delta=TOL,\n",
        "                               msg=\"Intercept drift\")\n",
        "\n",
        "        self.assertAlmostEqual(metrics[\"rmse\"], EXPECTED[\"rmse\"], delta=TOL,\n",
        "                               msg=\"RMSE drift\")\n",
        "        self.assertAlmostEqual(metrics[\"mse\"], EXPECTED[\"mse\"], delta=TOL,\n",
        "                               msg=\"MSE drift\")\n",
        "        self.assertAlmostEqual(metrics[\"r2\"], EXPECTED[\"r2\"], delta=TOL,\n",
        "                               msg=\"R² drift\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=['first-arg-is-ignored', 'TestLinearRegression'], exit=False, verbosity=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq7Xo2TuEW2y",
        "outputId": "3b419b6b-999a-41f2-beb2-5572295305ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_california_exact_values (__main__.TestLinearRegression.test_california_exact_values) ... ok\n",
            "test_eval_returns_perfect_metrics (__main__.TestLinearRegression.test_eval_returns_perfect_metrics) ... ok\n",
            "test_train_learns_coefficients (__main__.TestLinearRegression.test_train_learns_coefficients) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.037s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Régression polynomiale\n",
        "\n",
        "Dans cet exercice, vous allez créer un modèle de *machine learning* pour **modéliser une relation non linéaire** entre une variable explicative `X` et une cible `y`.  \n",
        "Le but est de comparer la **régression linéaire** classique à une **régression polynomiale** de **degré *d*** (par défaut : **3**) obtenue via `PolynomialFeatures`.\n",
        "\n",
        "### Objectifs pédagogiques\n",
        "1. **Comprendre** les étapes essentielles d'un workflow de *machine learning* :  préparation des données, entraînement, validation et visualisation.  \n",
        "2. **Implémenter** les fonctions `train_linear`, `eval_linear`, `train_poly`, `eval_poly` et `main` afin de :  \n",
        "   - séparer le jeu de données en `train` / `validation` (`train_test_split`) ;  \n",
        "   - entraîner un modèle `LinearRegression` **et** un modèle polynomiale ;  \n",
        "   - calculer et afficher les métriques : **RMSE**, **MSE** et **R²**.\n",
        "3. **Interpréter** l'impact du degré du polynôme : détection du sous-apprentissage (degré trop faible) et du sur-apprentissage (degré trop élevé).\n",
        "\n",
        "### Jeu de données\n",
        "Nous utiliserons une fonction utilitaire pour générer un jeu de données :\n",
        "\n",
        "```python\n",
        "X, y = generate_polynomial_data(n_samples=500, noise=3, random_state=0)\n",
        "```"
      ],
      "metadata": {
        "id": "EXWxX_lrAasr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code fourni"
      ],
      "metadata": {
        "id": "rsEKjFU9-MVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_polynomial_data(n_samples: int, noise: int, random_state: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Génère un dataset synthétique suivant une relation polynomiale cubique bruitée.\n",
        "\n",
        "    La variable cible y est générée selon la formule :\n",
        "        y = 3 * x³ - 2 * x² + x + bruit\n",
        "\n",
        "    où :\n",
        "        - x est une variable explicative tirée uniformément dans l'intervalle [-2, 2].\n",
        "        - le bruit est un vecteur de valeurs aléatoires tirées d'une distribution normale centrée réduite,\n",
        "          multiplié par un coefficient `noise` qui contrôle son amplitude.\n",
        "\n",
        "    Cette construction permet de simuler un phénomène non linéaire avec une variabilité naturelle,\n",
        "    utile pour tester et entraîner des modèles de régression sur des données réalistes mais contrôlées.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples : int\n",
        "        Nombre d'échantillons à générer.\n",
        "    noise : int\n",
        "        Amplitude (écart-type) du bruit gaussien ajouté aux valeurs cibles.\n",
        "        Un bruit plus important rend la relation entre x et y moins précise.\n",
        "    random_state : int, optional (default=0)\n",
        "        Graine pour initialiser le générateur pseudo-aléatoire, afin de garantir la reproductibilité.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : np.ndarray, shape (n_samples, 1)\n",
        "        Données explicatives générées, uniformément réparties entre -2 et 2.\n",
        "    y : np.ndarray, shape (n_samples,)\n",
        "        Valeurs cibles calculées selon la fonction polynomiale bruitée.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    X = rng.uniform(-2, 2, size=(n_samples, 1))\n",
        "    x = X[:, 0]\n",
        "    y = 3 * x**3 - 2 * x**2 + x + noise * rng.normal(size=n_samples)\n",
        "    return X, y\n",
        "\n",
        "def plot_comparison(model_linear: LinearRegression, model_poly: LinearRegression, poly: PolynomialFeatures,\n",
        "                    X: np.ndarray, y: np.ndarray, degree: int):\n",
        "    \"\"\"\n",
        "    Trace les courbes de prédiction des modèles linéaire et polynomial sur une plage de valeurs,\n",
        "    ainsi que les données brutes pour comparaison.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.scatter(X, y, s=10, alpha=0.4, label=\"Données brutes\")\n",
        "\n",
        "    x_line = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)\n",
        "    y_line_linear = model_linear.predict(x_line)\n",
        "    x_line_poly = poly.transform(x_line)\n",
        "    y_line_poly = model_poly.predict(x_line_poly)\n",
        "\n",
        "    plt.plot(x_line, y_line_linear, color=\"blue\", lw=2, label=\"Régression linéaire\")\n",
        "    plt.plot(x_line, y_line_poly, color=\"red\", lw=2, label=f\"Régression polynomiale (deg {degree})\")\n",
        "    plt.xlabel(\"Variable explicative\")\n",
        "    plt.ylabel(\"Cible\")\n",
        "    plt.title(\"Comparaison: régression linéaire vs polynomiale\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9oWHAc_eCCyK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code à compléter"
      ],
      "metadata": {
        "id": "3F0OA7CE-Lh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_poly(X_train: np.ndarray, y_train: np.ndarray, degree: int = 2) -> Tuple[LinearRegression, PolynomialFeatures]:\n",
        "    \"\"\"\n",
        "    Crée et entraîne un modèle de régression polynomiale.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : np.ndarray\n",
        "        Matrice des variables explicatives d'entraînement,\n",
        "        de forme (n_samples, n_features).\n",
        "    y_train : np.ndarray\n",
        "        Vecteur des valeurs cibles d'entraînement,\n",
        "        de forme (n_samples,).\n",
        "    degree : int, optional\n",
        "        Degré du polynôme à utiliser pour la transformation des données\n",
        "        (par défaut 2).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : LinearRegression\n",
        "        Modèle de régression linéaire ajusté sur les caractéristiques polynomiales.\n",
        "    poly : PolynomialFeatures\n",
        "        Transformateur PolynomialFeatures utilisé pour générer les\n",
        "        caractéristiques polynomiales à partir des données d'entrée.\n",
        "    \"\"\"\n",
        "    # TODO: Implémenter l'entraînement du modèle de régression polynomiale\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def eval_poly(model: LinearRegression, poly: PolynomialFeatures, X_val: np.ndarray, y_val: np.ndarray) -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Évalue le modèle de régression polynomiale sur un ensemble de validation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : LinearRegression\n",
        "        Modèle de régression polynomiale préalablement entraîné.\n",
        "    poly : PolynomialFeatures\n",
        "        Transformateur PolynomialFeatures utilisé pour générer les caractéristiques polynomiales.\n",
        "    X_val : np.ndarray\n",
        "        Matrice des variables explicatives de validation,\n",
        "        de forme (n_samples, n_features).\n",
        "    y_val : np.ndarray\n",
        "        Vecteur des valeurs cibles de validation,\n",
        "        de forme (n_samples,).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    metrics : dict[str, float]\n",
        "        Dictionnaire contenant les métriques, où les clés sont :\n",
        "\n",
        "        - ``rmse`` : erreur quadratique moyenne racine.\n",
        "        - ``mse``  : erreur quadratique moyenne.\n",
        "        - ``r2``   : coefficient de détermination R².\n",
        "    \"\"\"\n",
        "    # TODO: Implémenter la validation du modèle de régression polynomiale\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def train_linear(X_train: np.ndarray, y_train: np.ndarray) -> LinearRegression:\n",
        "    \"\"\"\n",
        "    Entraîne un modèle de régression linéaire classique sur les données d'entraînement.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : np.ndarray\n",
        "        Matrice des variables explicatives d'entraînement,\n",
        "        de forme (n_samples, n_features).\n",
        "    y_train : np.ndarray\n",
        "        Vecteur des valeurs cibles d'entraînement,\n",
        "        de forme (n_samples,).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : LinearRegression\n",
        "        Modèle de régression linéaire ajusté sur les données.\n",
        "    \"\"\"\n",
        "    # TODO: Implémenter l'entraînement du modèle de régression linéaire\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def eval_linear(model: LinearRegression, X_val: np.ndarray, y_val: np.ndarray) -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Évalue le modèle de régression linéaire sur un ensemble de validation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : LinearRegression\n",
        "        Modèle de régression linéaire préalablement entraîné.\n",
        "    X_val : np.ndarray\n",
        "        Matrice des variables explicatives de validation,\n",
        "        de forme (n_samples, n_features).\n",
        "    y_val : np.ndarray\n",
        "        Vecteur des valeurs cibles de validation,\n",
        "        de forme (n_samples,).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    metrics : dict[str, float]\n",
        "        Dictionnaire contenant les métriques, où les clés sont :\n",
        "\n",
        "        - ``rmse`` : erreur quadratique moyenne racine.\n",
        "        - ``mse``  : erreur quadratique moyenne.\n",
        "        - ``r2``   : coefficient de détermination R².\n",
        "    \"\"\"\n",
        "    # TODO: Implémenter la validation du modèle de régression linéaire\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def main(test_ratio: float = 0.2, random_state: int = 0, degree: int = 2):\n",
        "    X, y = generate_polynomial_data(n_samples=500, noise=3)\n",
        "\n",
        "    # TODO: Séparer le jeu de données en ensembles d'entraînement et de validation\n",
        "\n",
        "    # TODO: Instancier et entraîner votre modèle de régression linéaire et effectuer la validation du modèle\n",
        "\n",
        "    # TODO: Instancier et entraîner votre modèle de régression polynomiale et effectuer la validation du modèle\n",
        "\n",
        "    # TODO: Afficher les métriques, les coefficients et l'ordonnée à l'origine\n",
        "    #       du modèle de régression linéaire\n",
        "\n",
        "    # TODO: Afficher les métriques, les coefficients et l'ordonnée à l'origine\n",
        "    #       du modèle de régression polynomiale\n",
        "\n",
        "    # TODO: Afficher les visualisations de comparaison des deux modèles\n",
        "    #       à l'aide de la fonction plot\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(degree=3)"
      ],
      "metadata": {
        "id": "GPxCxqfJ-R06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "98e6d026-1547-4d92-d12a-f34975b6ccb4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2730708215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2730708215.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(test_ratio, random_state, degree)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m#       à l'aide de la fonction plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "4ivbEmh1_h3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LIN_EXPECTED = {\n",
        "    \"rmse\": 5.4827,\n",
        "    \"mse\": 30.0603,\n",
        "    \"r2\": 0.7305,\n",
        "}\n",
        "\n",
        "POLY_EXPECTED = {\n",
        "    \"rmse\": 3.0354,\n",
        "    \"mse\": 9.2138,\n",
        "    \"r2\": 0.9174,\n",
        "}\n",
        "\n",
        "TOL = 1e-2\n",
        "\n",
        "class TestPolynomialRegression(unittest.TestCase):\n",
        "    def test_train_poly_perfect_fit(self):\n",
        "        \"\"\"Le modèle de degré 3 doit retrouver exactement la courbe cubique sans bruit.\"\"\"\n",
        "        rng = np.random.default_rng(42)\n",
        "        X = rng.uniform(-1.5, 1.5, size=(300, 1))\n",
        "        x = X[:, 0]\n",
        "        y = 3 * x**3 - 2 * x**2 + x\n",
        "\n",
        "        model, poly = train_poly(X, y, degree=3)\n",
        "        metrics = eval_poly(model, poly, X, y)\n",
        "\n",
        "        self.assertIsInstance(model, LinearRegression)\n",
        "        self.assertIsInstance(poly, PolynomialFeatures)\n",
        "\n",
        "        self.assertAlmostEqual(metrics[\"rmse\"], 0.0, delta=1e-9)\n",
        "        self.assertAlmostEqual(metrics[\"mse\"],  0.0, delta=1e-9)\n",
        "        self.assertAlmostEqual(metrics[\"r2\"],   1.0, delta=1e-9)\n",
        "\n",
        "    def test_polynomial_beats_linear_on_synthetic_data(self):\n",
        "        \"\"\"Sur un jeu de données non linéaire bruité, la régression polynomiale\n",
        "        doit surpasser la régression linéaire (meilleur R², plus petit RMSE).\"\"\"\n",
        "        X, y = generate_polynomial_data(n_samples=500, noise=3, random_state=0)\n",
        "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=0, shuffle=True\n",
        "        )\n",
        "\n",
        "        lin_model  = train_linear(X_tr, y_tr)\n",
        "        lin_metrics = eval_linear(lin_model, X_val, y_val)\n",
        "\n",
        "        poly_model, poly = train_poly(X_tr, y_tr, degree=3)\n",
        "        poly_metrics = eval_poly(poly_model, poly, X_val, y_val)\n",
        "\n",
        "        self.assertLess(poly_metrics[\"rmse\"], lin_metrics[\"rmse\"],\n",
        "                        msg=\"Le modèle polynomial devrait réduire le RMSE.\")\n",
        "        self.assertGreater(poly_metrics[\"r2\"], lin_metrics[\"r2\"],\n",
        "                           msg=\"Le modèle polynomial devrait augmenter le R².\")\n",
        "\n",
        "        for key, expected in LIN_EXPECTED.items():\n",
        "            self.assertAlmostEqual(lin_metrics[key], expected, delta=TOL,\n",
        "                                   msg=f\"Dérive détectée pour {key} (linéaire).\")\n",
        "        for key, expected in POLY_EXPECTED.items():\n",
        "            self.assertAlmostEqual(poly_metrics[key], expected, delta=TOL,\n",
        "                                   msg=f\"Dérive détectée pour {key} (polynomial).\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=[\"first-arg-is-ignored\", \"TestPolynomialRegression\"], exit=False, verbosity=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dix813CL_j5g",
        "outputId": "a4c4142d-63a8-4945-d6b8-eb8a7fc8e8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_polynomial_beats_linear_on_synthetic_data (__main__.TestPolynomialRegression.test_polynomial_beats_linear_on_synthetic_data)\n",
            "Sur un jeu de données non linéaire bruité, la régression polynomiale ... ok\n",
            "test_train_poly_perfect_fit (__main__.TestPolynomialRegression.test_train_poly_perfect_fit)\n",
            "Le modèle de degré 3 doit retrouver exactement la courbe cubique sans bruit. ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.019s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Régression logistique\n",
        "\n",
        "Dans cet exercice, vous allez construire un modèle de *machine learning* pour **prédire si une tumeur est maligne (1) ou bénigne (0)** à partir de deux mesures morphologiques : `mean radius` et `mean texture`.  \n",
        "Le modèle choisi est la **régression logistique**, qui est bien adaptée aux problèmes de classification binaire.\n",
        "\n",
        "### Objectifs pédagogiques\n",
        "1. **Comprendre** les étapes clés d'un workflow de classification :  \n",
        "   préparation des données, normalisation, entraînement, validation et visualisation.  \n",
        "2. **Implémenter** les fonctions `train`, `eval` et `main` afin de :  \n",
        "   - séparer le jeu de données en `train` / `validation` (`train_test_split`) ;  \n",
        "   - normaliser les variables explicatives avec `StandardScaler` ;  \n",
        "   - entraîner un modèle `LogisticRegression` de *scikit-learn* ;  \n",
        "   - calculer et afficher les métriques : **Accuracy**, **Precision**, **Recall**, **F1** et la **matrice de confusion**.  \n",
        "3. **Analyser** les performances du modèle : examiner la **matrice de confusion** et les **métriques** (Accuracy, Precision, Recall, F1) afin de repérer faux positifs / faux négatifs et juger la qualité globale de la classification.\n",
        "\n",
        "### Jeu de données\n",
        "Nous utiliserons le **Breast Cancer Wisconsin Dataset** fourni par *scikit-learn*.\n",
        "\n",
        "*Référence utile : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html*"
      ],
      "metadata": {
        "id": "gM3EkSfwAjFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code fourni"
      ],
      "metadata": {
        "id": "0AsX8oTHJV9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(model: LogisticRegression, scaler: StandardScaler, X: np.ndarray, y: np.ndarray):\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.scatter(\n",
        "        X[:, 0], X[:, 1],\n",
        "        c=y, cmap=\"bwr\", alpha=0.6, edgecolor=\"k\", s=35, label=\"Données\"\n",
        "    )\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.linspace(x_min, x_max, 300),\n",
        "        np.linspace(y_min, y_max, 300)\n",
        "    )\n",
        "    grid_std = scaler.transform(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = model.predict(grid_std).reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, levels=[-1, 0, 1], alpha=0.15, cmap=\"bwr\")\n",
        "    plt.xlabel(\"Rayon moyen\")\n",
        "    plt.ylabel(\"Texture moyenne\")\n",
        "    plt.title(\"Cancer du sein - Régression logistique\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qAIzEg15JaFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code à compléter"
      ],
      "metadata": {
        "id": "IhRTwLvRJXyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train: np.ndarray, y_train: np.ndarray) -> LogisticRegression:\n",
        "    \"\"\"\n",
        "    Crée et entraîne un modèle de régression logistique sur l'ensemble d'entraînement.\n",
        "\n",
        "    Le modèle utilise le solver 'lbfgs' avec un nombre maximal d'itérations fixé à 1000\n",
        "    pour assurer la convergence.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : np.ndarray\n",
        "        Matrice des variables explicatives d'entraînement,\n",
        "        de forme (n_samples, n_features).\n",
        "    y_train : np.ndarray\n",
        "        Vecteur des labels cibles d'entraînement,\n",
        "        de forme (n_samples,).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : LogisticRegression\n",
        "        Modèle de régression logistique entraîné.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Effectuer la régression logistique sur l'ensemble d'entraînement\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def eval(model: LogisticRegression, X_val: np.ndarray, y_val: np.ndarray) -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Évalue les performances du modèle de régression logistique sur l'ensemble de validation.\n",
        "\n",
        "    Les métriques retournées sont l'Accuracy, la Precision, le Recall et le F1-score.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : LogisticRegression\n",
        "        Modèle de régression logistique entraîné.\n",
        "    X_val : np.ndarray\n",
        "        Matrice des variables explicatives de validation,\n",
        "        de forme (n_samples, n_features).\n",
        "    y_val : np.ndarray\n",
        "        Vecteur des labels cibles de validation,\n",
        "        de forme (n_samples,).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    metrics : dict[str, float]\n",
        "        Dictionnaire contenant les métriques suivantes, où les clés sont:\n",
        "\n",
        "        - ``accuracy`` : précision globale.\n",
        "        - ``precision`` : précision positive.\n",
        "        - ``recall`` : rappel.\n",
        "        - ``f1`` : score F1.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Effectuer la validation du modèle sur l'ensemble de validation\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def main(test_ratio: float = 0.2, random_state: int = 42):\n",
        "    data = load_breast_cancer()\n",
        "    features = [\"mean radius\", \"mean texture\"]\n",
        "    idx = [list(data.feature_names).index(f) for f in features]\n",
        "    X = data.data[:, idx]\n",
        "    y = data.target\n",
        "\n",
        "    # TODO: Séparer le jeu de données en ensemble d'entraînement et de validation\n",
        "\n",
        "    # TODO: Normaliser vos variables explicatives\n",
        "\n",
        "    # TODO: Instancier et entraîner votre modèle et effectuer la validation du modèle\n",
        "\n",
        "    # TODO: Afficher les métriques et la matrice de confusion du modèle\n",
        "\n",
        "    # TODO: Afficher les visualisations à partir de la fonction plot\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "aDAAKHFiJa1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "ec189d9a-a30a-4dd8-b3e6-c708a3cc45fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-690533865.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-690533865.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(test_ratio, random_state)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# TODO: Afficher les visualisations à partir de la fonction plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "0V_z2B6wJbTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPECTED = {\n",
        "    \"Accuracy\":  0.8684,\n",
        "    \"Precision\": 0.9014,\n",
        "    \"Recall\":    0.8889,\n",
        "    \"F1\":        0.8951,\n",
        "    \"conf_mat\":  np.array([[35, 7],\n",
        "                           [ 8, 64]]),\n",
        "}\n",
        "\n",
        "TOL = 1e-2\n",
        "\n",
        "class TestLogisticRegression(unittest.TestCase):\n",
        "    def test_train_perfect_separation(self):\n",
        "        rng = np.random.default_rng(0)\n",
        "        X_pos = rng.normal(loc=+2.0, scale=0.2, size=(50, 2))\n",
        "        X_neg = rng.normal(loc=-2.0, scale=0.2, size=(50, 2))\n",
        "        X = np.vstack([X_pos, X_neg])\n",
        "        y = np.hstack([np.ones(50), np.zeros(50)])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_std = scaler.fit_transform(X)\n",
        "\n",
        "        model = train(X_std, y)\n",
        "        metrics = eval(model, X_std, y)\n",
        "\n",
        "        self.assertIsInstance(model, LogisticRegression)\n",
        "        for m in (\"Accuracy\", \"Precision\", \"Recall\", \"F1\"):\n",
        "            self.assertAlmostEqual(metrics[m], 1.0, delta=1e-6,\n",
        "                                   msg=f\"{m} should be 1.0 on separable data\")\n",
        "\n",
        "    def test_breast_cancer_expected_metrics(self):\n",
        "        data = load_breast_cancer()\n",
        "        features = [\"mean radius\", \"mean texture\"]\n",
        "        idx = [list(data.feature_names).index(f) for f in features]\n",
        "        X = data.data[:, idx]\n",
        "        y = data.target\n",
        "\n",
        "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=0, stratify=y\n",
        "        )\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_tr_std = scaler.fit_transform(X_tr)\n",
        "        X_val_std = scaler.transform(X_val)\n",
        "\n",
        "        model = train(X_tr_std, y_tr)\n",
        "        metrics = eval(model, X_val_std, y_val)\n",
        "\n",
        "        for key in (\"Accuracy\", \"Precision\", \"Recall\", \"F1\"):\n",
        "            self.assertAlmostEqual(metrics[key], EXPECTED[key], delta=TOL,\n",
        "                                   msg=f\"Dérive détectée pour {key}\")\n",
        "\n",
        "        conf = confusion_matrix(y_val, model.predict(X_val_std))\n",
        "        self.assertTrue(np.array_equal(conf, EXPECTED[\"conf_mat\"]),\n",
        "                        msg=\"Matrice de confusion différente des valeurs de référence\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=[\"first-arg-is-ignored\", \"TestLogisticRegression\"], exit=False, verbosity=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsuld5HNJdp1",
        "outputId": "d507ede6-8f0b-4a32-8a34-57115de44bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_breast_cancer_expected_metrics (__main__.TestLogisticRegression.test_breast_cancer_expected_metrics) ... ok\n",
            "test_train_perfect_separation (__main__.TestLogisticRegression.test_train_perfect_separation) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.051s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Réseaux de neurones\n",
        "\n",
        "Dans cet exercice, vous allez implémenter un **réseau de neurones** pour effectuer une tâche de **classification d'images** sur le dataset MNIST, qui contient des images de chiffres manuscrits (0 à 9).\n",
        "\n",
        "Le modèle utilisé est un **réseau de neurones entièrement connecté** (MLP - Multi-Layer Perceptron), construit avec **PyTorch**. Il est composé de plusieurs couches linéaires avec des fonctions d'activation non-linéaires.\n",
        "\n",
        "### Objectifs pédagogiques\n",
        "\n",
        "Apprendre les étapes clés d'un workflow de deep learning :\n",
        "\n",
        "- Charger et transformer les données MNIST (`transforms.ToTensor`)\n",
        "- Construire un réseau de neurones avec `torch.nn.Module`\n",
        "- Entraîner (`train`) et évaluer (`eval`) le modèle avec la perte `CrossEntropyLoss` et l'optimiseur (`Adam`)\n",
        "- Mesurer les performances (Accuracy)\n",
        "- Visualiser les prédictions sur des exemples d'images\n",
        "\n",
        "Vous implémenterez les fonctions `train`, `eval` et `main` pour réaliser ce processus de bout en bout. L'entraînement sera effectué sur 5 epochs.\n",
        "\n",
        "### Jeu de données\n",
        "\n",
        "Nous utiliserons le dataset **MNIST** fourni par `torchvision.datasets`.\n",
        "\n",
        "*Référence utile : https://docs.pytorch.org/tutorials/beginner/basics/intro.html*\n"
      ],
      "metadata": {
        "id": "CmRBeV2xAlhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code fourni"
      ],
      "metadata": {
        "id": "4eHsndNPBI0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_examples(images, labels, preds=None, n=6):\n",
        "    plt.figure(figsize=(10, 2))\n",
        "\n",
        "    for i in range(n):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.imshow(images[i][0], cmap=\"gray\")\n",
        "        title = f\"{labels[i]}\"\n",
        "\n",
        "        if preds is not None:\n",
        "            title += f\"→{preds[i]}\"\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YAaRl0hYBGcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code à compléter"
      ],
      "metadata": {
        "id": "Wz3S_HsRBLSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Réseau de neurones entièrement connecté (MLP) pour la classification MNIST.\n",
        "\n",
        "    Architecture :\n",
        "    - nn.Flatten() : aplatissement des images 28x28 en vecteurs 1D de taille 784.\n",
        "    - Couche linéaire 1 : 784 → 128 neurones.\n",
        "    - ReLU : fonction d'activation non linéaire.\n",
        "    - Couche linéaire 2 : 128 → 64 neurones.\n",
        "    - ReLU : fonction d'activation non linéaire.\n",
        "    - Couche linéaire 3 : 64 → 10 neurones (logits pour chaque classe MNIST).\n",
        "\n",
        "    Cette architecture permet d'apprendre des représentations successives de plus en plus abstraites,\n",
        "    adaptées à la classification des chiffres manuscrits.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: Implémenter l'architecture du réseau de neurones\n",
        "        self.net = None\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Propagation avant du réseau.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Batch d'images d'entrée, de forme (batch_size, 1, 28, 28).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Logits de sortie, de forme (batch_size, 10).\n",
        "        \"\"\"\n",
        "        # TODO: Implémenter la propagation avant du réseau\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "def train(model, loader, criterion, optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Entraîne le modèle pour un epoch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Modèle à entraîner.\n",
        "    loader : DataLoader\n",
        "        Itérateur fournissant les batches d'entraînement.\n",
        "    criterion : torch.nn.Module\n",
        "        Fonction de perte (CrossEntropyLoss).\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        Optimiseur (Adam).\n",
        "    epoch : int\n",
        "        Numéro de l'epoch en cours (affichage).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # TODO: Implémenter l'entraînement de votre modèle\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "def eval(model, loader, criterion):\n",
        "    \"\"\"\n",
        "    Évalue le modèle sur un ensemble de validation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Modèle entraîné.\n",
        "    loader : DataLoader\n",
        "        Itérateur fournissant les batches de validation.\n",
        "    criterion : torch.nn.Module\n",
        "        Fonction de perte utilisée (CrossEntropyLoss).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_true : np.ndarray\n",
        "        Labels vrais concaténés sur tout l'ensemble.\n",
        "    y_pred : np.ndarray\n",
        "        Prédictions concaténées sur tout l'ensemble.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # TODO: Implémenter la validation de votre modèle\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def main(batch_size=128, epochs=5, lr=1e-3, random_state=0):\n",
        "    torch.manual_seed(random_state)\n",
        "\n",
        "    transform = transforms.ToTensor()\n",
        "\n",
        "    train_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "    test_ds  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # TODO: Instancier votre modèle, votre perte (CrossEntropy Loss) et votre optimiseur (Adam)\n",
        "\n",
        "    # TODO: Effectuer l'entraînement et la validation de votre modèle\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "    sample_imgs, sample_labels = next(iter(test_loader))[:6]\n",
        "    sample_preds = model(sample_imgs.to(DEVICE)).argmax(1).cpu()\n",
        "    plot_examples(sample_imgs, sample_labels, sample_preds)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "dH16CEVTAqMD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "356d9c13-ec37-46b4-86d2-7ba8d26c1c84"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 19.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 609kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 5.59MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.74MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1866571061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1866571061.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(batch_size, epochs, lr, random_state)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# TODO: Effectuer l'entraînement et la validation de votre modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0msample_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "mjQUYT-0M15s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOL = 1e-3\n",
        "\n",
        "class TestNeuralNetworks(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.model = NeuralNet().to(DEVICE)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "\n",
        "        x = torch.randn(10, 1, 28, 28).to(DEVICE)\n",
        "        y = torch.arange(10).to(DEVICE)\n",
        "        self.loader = DataLoader(TensorDataset(x, y), batch_size=5)\n",
        "\n",
        "    def test_forward_output_shape(self):\n",
        "        for xb, _ in self.loader:\n",
        "            out = self.model(xb)\n",
        "            self.assertEqual(out.shape, (xb.size(0), 10))\n",
        "\n",
        "    def test_train_reduces_loss(self):\n",
        "        xb, yb = next(iter(self.loader))\n",
        "        self.model.train()\n",
        "        initial_loss = self.criterion(self.model(xb), yb).item()\n",
        "\n",
        "        train(self.model, self.loader, self.criterion, self.optimizer, epoch=1)\n",
        "\n",
        "        new_loss = self.criterion(self.model(xb), yb).item()\n",
        "        self.assertLessEqual(new_loss, initial_loss + TOL)\n",
        "\n",
        "    def test_eval_returns_correct_format(self):\n",
        "        y_true, y_pred = eval(self.model, self.loader, self.criterion)\n",
        "        self.assertIsInstance(y_true, np.ndarray)\n",
        "        self.assertIsInstance(y_pred, np.ndarray)\n",
        "        self.assertEqual(len(y_true), len(y_pred))\n",
        "        self.assertTrue(((y_pred >= 0) & (y_pred < 10)).all())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(\n",
        "        argv=[\"first-arg-is-ignored\", \"TestNeuralNetworks\"],\n",
        "        exit=False,\n",
        "        verbosity=2,\n",
        "    )"
      ],
      "metadata": {
        "id": "DBINYQs6M37c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28819370-bb73-49e2-a1b8-5c251633cc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_eval_returns_correct_format (__main__.TestNeuralNetworks.test_eval_returns_correct_format) ... ok\n",
            "test_forward_output_shape (__main__.TestNeuralNetworks.test_forward_output_shape) ... ok\n",
            "test_train_reduces_loss (__main__.TestNeuralNetworks.test_train_reduces_loss) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.073s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation - loss: 2.2866 | acc: 0.0000\n",
            "[Epoch 1] train loss: 2.2903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arbres de décision\n",
        "\n",
        "\n",
        "Dans cet exercice, vous allez comparer plusieurs **modèles à base d'arbres de décision** pour prédire le **prix médian des maisons en Californie**.  \n",
        "L'idée est de mesurer l'apport de techniques plus sophistiquées (forêt aléatoire, gradient boosting) par rapport à une **régression linéaire** de référence.\n",
        "\n",
        "### Objectifs pédagogiques\n",
        "1. **Comprendre** les étapes essentielles d'un workflow de *machine learning* :  préparation des données, entraînement, validation et visualisation.\n",
        "2. **Implémenter** les fonctions génériques `train`, `eval` et `main` pour :  \n",
        "   - instancier plusieurs modèles (`LinearRegression`, `DecisionTreeRegressor`, `RandomForestRegressor`, `GradientBoostingRegressor`) ;  \n",
        "   - calculer et afficher les métriques : **RMSE**, **MSE** et **R²** ;  \n",
        "   - tracer un nuage **valeurs réelles vs prédictions** pour chaque modèle.  \n",
        "3. **Comparer** l'impact de la complexité du modèle :  \n",
        "   - montrer le gain de performance des modèles d'ensemble.\n",
        "\n",
        "### Jeu de données\n",
        "Nous utiliserons le **California Housing Dataset** fourni par *scikit-learn*.\n",
        "\n",
        "*Références utiles*\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
        "\n"
      ],
      "metadata": {
        "id": "M7d_dPOqAqwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code fourni"
      ],
      "metadata": {
        "id": "w9SnU6FMCLJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(y_true: np.ndarray, y_pred: np.ndarray, title: str):\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.scatter(y_true, y_pred, s=10, alpha=0.5)\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], color=\"red\", lw=2, label=\"y = ŷ\")\n",
        "    plt.xlabel(\"Valeurs réelles\")\n",
        "    plt.ylabel(\"Valeurs prédites\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_model(name: str, model_class, X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, **kwargs):\n",
        "    print(f\"\\n----- {name} -----\")\n",
        "    model = train(model_class, X_train, y_train, **kwargs)\n",
        "    results = eval(model, X_val, y_val)\n",
        "\n",
        "    print(f\"Validation RMSE : {results['rmse']:.4f}\")\n",
        "    print(f\"Validation MSE  : {results['mse']:.4f}\")\n",
        "    print(f\"Validation R²   : {results['r2']:.4f}\")\n",
        "\n",
        "    plot(y_val, results[\"y_pred\"], f\"Prédictions vs Réel - {name}\")"
      ],
      "metadata": {
        "id": "3HnlKxqfCKYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code à compléter"
      ],
      "metadata": {
        "id": "7cHwjdXkCUVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model_class, X_train: np.ndarray, y_train: np.ndarray, **kwargs):\n",
        "    \"\"\"\n",
        "    Crée et entraîne un modèle de régression donné sur les données d'entraînement.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_class : class\n",
        "        Classe du modèle à instancier (ex: LinearRegression, DecisionTreeRegressor).\n",
        "    X_train : np.ndarray\n",
        "        Matrice des variables explicatives d'entraînement, forme (n_samples, n_features).\n",
        "    y_train : np.ndarray\n",
        "        Vecteur des cibles d'entraînement, forme (n_samples,).\n",
        "    **kwargs :\n",
        "        Arguments optionnels à passer au constructeur du modèle.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : objet\n",
        "        Modèle entraîné.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implémenter l'entraînement du modèle\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def eval(model, X_val: np.ndarray, y_val: np.ndarray) -> dict[str, float | np.ndarray]:\n",
        "    \"\"\"\n",
        "    Évalue un modèle sur un ensemble de validation.\n",
        "\n",
        "    Calcule les métriques RMSE, MSE et coefficient de détermination R²,\n",
        "    et génère les prédictions correspondantes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : objet\n",
        "        Modèle entraîné avec une méthode predict.\n",
        "    X_val : np.ndarray\n",
        "        Matrice des variables explicatives de validation.\n",
        "    y_val : np.ndarray\n",
        "        Vecteur des cibles de validation.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict[str, float | np.ndarray]\n",
        "        Dictionnaire contenant les métriques et les prédictions, où les clés sont:\n",
        "        - 'rmse' (float) : racine de l'erreur quadratique moyenne,\n",
        "        - 'mse' (float) : erreur quadratique moyenne,\n",
        "        - 'r2' (float) : coefficient de détermination,\n",
        "        - 'y_pred' (np.ndarray) : prédictions sur X_val.\n",
        "    \"\"\"\n",
        "    # TODO: Implémenter la validation du modèle\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def main(test_ratio: float = 0.2, random_state: int = 0):\n",
        "    data = fetch_california_housing()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "\n",
        "    # TODO: Séparer votre jeu de données en ensembles d'entraînement et de validation\n",
        "\n",
        "    # TODO: Instancier, entraîner et valider les modèles suivants à partir de la fonction create_model:\n",
        "    #       Régression linéaire, Arbre de décision, Forêt aléatoire, Gradient boosting\n",
        "    #       Indice: Passez la classe du modèle (et non une instance) comme deuxième argument de\n",
        "    #       create_model.  La fonction se chargera de l’instancier via train(...).\n",
        "    #       Exemple : create_model(\"Régression linéaire\", LinearRegression, ...).\n",
        "    #       Ajoutez les hyperparamètres éventuels dans kwargs (après X_val, y_val).\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "T3RfONHmCWj6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "99528f21-1b9d-4123-82a2-3348ca7ae189"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3303734707.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3303734707.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(test_ratio, random_state)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m#       Ajoutez les hyperparamètres éventuels dans kwargs (après X_val, y_val).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "_nXkomwACeDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPECTED = {\n",
        "    \"LinearRegression\": {\"rmse\": 0.7273, \"mse\": 0.5290, \"r2\": 0.5943},\n",
        "    \"DecisionTree\":     {\"rmse\": 0.7290, \"mse\": 0.5315, \"r2\": 0.5924},\n",
        "    \"RandomForest\":     {\"rmse\": 0.5131, \"mse\": 0.2633, \"r2\": 0.7981},\n",
        "    \"GradientBoosting\": {\"rmse\": 0.5388, \"mse\": 0.2903, \"r2\": 0.7774},\n",
        "}\n",
        "\n",
        "TOL = 1e-2\n",
        "\n",
        "class TestTreeBasedModels(unittest.TestCase):\n",
        "    def test_linear_regression_perfect_fit(self):\n",
        "        rng = np.random.default_rng(1)\n",
        "        X = rng.random((80, 3))\n",
        "        coeff = rng.random(3)\n",
        "        y = X @ coeff\n",
        "        model = train(LinearRegression, X, y)\n",
        "        metrics = eval(model, X, y)\n",
        "        for key in (\"rmse\", \"mse\"):\n",
        "            self.assertAlmostEqual(metrics[key], 0.0, delta=TOL)\n",
        "        self.assertAlmostEqual(metrics[\"r2\"], 1.0, delta=TOL)\n",
        "\n",
        "    def test_decision_tree_perfect_fit(self):\n",
        "        rng = np.random.default_rng(2)\n",
        "        X = rng.random((100, 2))\n",
        "        y = rng.random(100)\n",
        "        model = train(DecisionTreeRegressor, X, y, random_state=0)\n",
        "        metrics = eval(model, X, y)\n",
        "        for key in (\"rmse\", \"mse\"):\n",
        "            self.assertAlmostEqual(metrics[key], 0.0, delta=TOL)\n",
        "        self.assertAlmostEqual(metrics[\"r2\"], 1.0, delta=TOL)\n",
        "\n",
        "    def test_random_forest_perfect_fit(self):\n",
        "        rng = np.random.default_rng(3)\n",
        "        X = rng.random((60, 4))\n",
        "        y = rng.random(60)\n",
        "        model = train(\n",
        "            RandomForestRegressor,\n",
        "            X,\n",
        "            y,\n",
        "            n_estimators=1,\n",
        "            bootstrap=False,\n",
        "            random_state=0,\n",
        "        )\n",
        "        metrics = eval(model, X, y)\n",
        "        for key in (\"rmse\", \"mse\"):\n",
        "            self.assertAlmostEqual(metrics[key], 0.0, delta=TOL)\n",
        "        self.assertAlmostEqual(metrics[\"r2\"], 1.0, delta=TOL)\n",
        "\n",
        "    def test_gradient_boosting_perfect_fit(self):\n",
        "        rng = np.random.default_rng(4)\n",
        "        X = rng.random((50, 2))\n",
        "        y = rng.random(50)\n",
        "        model = train(\n",
        "            GradientBoostingRegressor,\n",
        "            X,\n",
        "            y,\n",
        "            n_estimators=500,\n",
        "            learning_rate=1.0,\n",
        "            max_depth=3,\n",
        "            random_state=0,\n",
        "        )\n",
        "        metrics = eval(model, X, y)\n",
        "        for key in (\"rmse\", \"mse\"):\n",
        "            self.assertAlmostEqual(metrics[key], 0.0, delta=TOL)\n",
        "        self.assertAlmostEqual(metrics[\"r2\"], 1.0, delta=TOL)\n",
        "\n",
        "    def test_expected_metrics_california(self):\n",
        "        data = fetch_california_housing()\n",
        "        X, y = data.data, data.target\n",
        "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "            X,\n",
        "            y,\n",
        "            test_size=0.2,\n",
        "            random_state=0,\n",
        "            shuffle=True,\n",
        "        )\n",
        "        suite = [\n",
        "            (\"LinearRegression\", LinearRegression, {}),\n",
        "            (\"DecisionTree\", DecisionTreeRegressor, {\"random_state\": 0}),\n",
        "            (\n",
        "                \"RandomForest\",\n",
        "                RandomForestRegressor,\n",
        "                {\"n_estimators\": 100, \"random_state\": 0},\n",
        "            ),\n",
        "            (\n",
        "                \"GradientBoosting\",\n",
        "                GradientBoostingRegressor,\n",
        "                {\"n_estimators\": 100, \"random_state\": 0},\n",
        "            ),\n",
        "        ]\n",
        "        for name, cls, kwargs in suite:\n",
        "            model = train(cls, X_tr, y_tr, **kwargs)\n",
        "            metrics = eval(model, X_val, y_val)\n",
        "            for key in (\"rmse\", \"mse\", \"r2\"):\n",
        "                self.assertAlmostEqual(metrics[key], EXPECTED[name][key], delta=TOL)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(\n",
        "        argv=[\"first-arg-is-ignored\", \"TestTreeBasedModels\"],\n",
        "        exit=False,\n",
        "        verbosity=2,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4avEBsRqWY_k",
        "outputId": "e3f4ceb3-48e0-4185-cb56-e8e67176b7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_decision_tree_perfect_fit (__main__.TestTreeBasedModels.test_decision_tree_perfect_fit) ... ok\n",
            "test_expected_metrics_california (__main__.TestTreeBasedModels.test_expected_metrics_california) ... ok\n",
            "test_gradient_boosting_perfect_fit (__main__.TestTreeBasedModels.test_gradient_boosting_perfect_fit) ... ok\n",
            "test_linear_regression_perfect_fit (__main__.TestTreeBasedModels.test_linear_regression_perfect_fit) ... ok\n",
            "test_random_forest_perfect_fit (__main__.TestTreeBasedModels.test_random_forest_perfect_fit) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 26.950s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    }
  ]
}